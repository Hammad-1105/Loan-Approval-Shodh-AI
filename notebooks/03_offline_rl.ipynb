{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25eccbfd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226492ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import d3rlpy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9405feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "d3rlpy.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597ddc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/processed_loans.csv'\n",
    "df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387a685",
   "metadata": {},
   "source": [
    "# Reward Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba43192b",
   "metadata": {},
   "source": [
    "<p>Logic for reward engineering:<br>\n",
    "1. Action = Deny: Reward = 0 (We will generate these later in Step 2)<br>\n",
    "2. Action = Approve & Fully Paid: Reward = + (loan_amnt * int_rate) (Profit)<br>\n",
    "3. Action = Approve & Default: Reward = - loan_amnt (Loss)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270109a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculated_reward(row):\n",
    "    if row['target'] == 0: # Fully paid\n",
    "        return row['loan_amnt'] * (row['int_rate']/100.0) # Reward\n",
    "    \n",
    "    else: # Default\n",
    "        return -1.0 * row['loan_amnt'] # Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66770b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reward'] = df.apply(calculated_reward, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09dc318",
   "metadata": {},
   "source": [
    "## Financial inspection based upon rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6cb94a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Statistics: \n",
      "count    176083.000000\n",
      "mean      -1805.502019\n",
      "std        8039.184595\n",
      "min      -35000.000000\n",
      "25%         380.700000\n",
      "50%         983.200000\n",
      "75%        1893.600000\n",
      "max       10146.500000\n",
      "Name: reward, dtype: float64\n",
      "\n",
      "Max Profit (Best Loan):   $10146.50\n",
      "Max Loss (Worst Loan):    $-35000.00\n",
      "Total Portfolio Value:    $-317,918,212.10\n"
     ]
    }
   ],
   "source": [
    "print(\"Reward Statistics: \")\n",
    "print(df['reward'].describe())\n",
    "\n",
    "print(f\"\\nMax Profit (Best Loan):   ${df['reward'].max():.2f}\")\n",
    "print(f\"Max Loss (Worst Loan):    ${df['reward'].min():.2f}\")\n",
    "print(f\"Total Portfolio Value:    ${df['reward'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc2a90",
   "metadata": {},
   "source": [
    "<p>The historical strategy (approving these specific people) was a financial disaster. Even though ~80% of people paid back, the 20% who defaulted caused such massive losses (up to -$35,000 each) that they wiped out all the profit from the good loans.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9788d",
   "metadata": {},
   "source": [
    "<p>The DL Model (from Phase 2) learned to mimic this history. It would approve almost everyone to get high accuracy.\n",
    "\n",
    "Result: The DL model would lose the company $317 Million.\n",
    "\n",
    "The RL Agent's Job: It needs to look at this mess and learn to say \"NO\" (Action 0). If it simply denied everyone, the profit would be $0, which is effectively a $317 Million improvement over the current strategy!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde8aa3",
   "metadata": {},
   "source": [
    "# MDP Construction & Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7496cb6",
   "metadata": {},
   "source": [
    "<p>Create the \"Training Data\" for the RL agent. We need to teach the agent that Denying (Action 0) is a valid option that yields $0 Reward.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd441b",
   "metadata": {},
   "source": [
    "Batch A (The Reality): The actual loans. <br>\n",
    "State: Applicant Features <br>\n",
    "Action: 1 (Approve) <br>\n",
    "Reward: -$1,805 (on average) <br>\n",
    "\n",
    "Batch B (The Simulation): The same applicants, but hypothetically denied. <br>\n",
    "State: Applicant Features <br>\n",
    "Action: 0 (Deny) <br>\n",
    "Reward: 0 (Risk-Free) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec8f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df.drop(columns=['target', 'reward']).columns\n",
    "X = df[feature_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71dd4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68f88231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (176083, 79)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d283268",
   "metadata": {},
   "source": [
    "## Batch A: \"Real Approvals\" (Action = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146278e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_approve = X_scaled\n",
    "act_approve = np.ones(len(df), dtype=int) # Action 1\n",
    "rew_approve = df['reward'].values         # Real Profit/Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b00653",
   "metadata": {},
   "source": [
    "## Batch B: \"Synthetic Denials\" (Action = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ff4349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_deny = X_scaled\n",
    "act_deny = np.zeros(len(df), dtype=int)   # Action 0\n",
    "rew_deny = np.zeros(len(df), dtype=float) # Reward 0 (Safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80918ba",
   "metadata": {},
   "source": [
    "## Combining A & B for Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaa9cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.concatenate([obs_approve, obs_deny], axis=0)\n",
    "actions = np.concatenate([act_approve, act_deny], axis=0)\n",
    "rewards = np.concatenate([rew_approve, rew_deny], axis=0)\n",
    "terminals = np.ones(len(observations), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "374b77c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:38.12 [info     ] Signatures have been automatically determined. action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]) observation_signature=Signature(dtype=[dtype('float64')], shape=[(79,)]) reward_signature=Signature(dtype=[dtype('float64')], shape=[(1,)])\n",
      "2025-12-09 12:38.12 [info     ] Action-space has been automatically determined. action_space=<ActionSpace.DISCRETE: 2>\n",
      "2025-12-09 12:38.13 [info     ] Action size has been automatically determined. action_size=2\n"
     ]
    }
   ],
   "source": [
    "dataset = d3rlpy.dataset.MDPDataset(\n",
    "    observations=observations,\n",
    "    actions=actions,\n",
    "    rewards=rewards,\n",
    "    terminals=terminals,\n",
    "    #discrete_action=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08b12560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Transitions: 352166\n",
      "Action Distribution: (array([0, 1]), array([176083, 176083]))\n",
      "Average Reward in Dataset: -902.75\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Transitions: {len(observations)}\")\n",
    "print(f\"Action Distribution: {np.unique(actions, return_counts=True)}\")\n",
    "print(f\"Average Reward in Dataset: {np.mean(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae8686",
   "metadata": {},
   "source": [
    "# Training the RL Agent (CQL based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0f6ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cql = d3rlpy.algos.DiscreteCQLConfig(\n",
    "    batch_size=256,\n",
    "    learning_rate=1e-4,\n",
    "    alpha=1.0 # Controls how \"conservative\" the agent is \n",
    ").create(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35e9773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:45.17 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('float64')], shape=[(79,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('float64')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=2)\n",
      "2025-12-09 12:45.17 [debug    ] Building models...            \n",
      "2025-12-09 12:45.18 [debug    ] Models have been built.       \n",
      "2025-12-09 12:45.18 [info     ] Directory is created at d3rlpy_logs\\loan_cql_run_20251209124518\n",
      "2025-12-09 12:45.18 [info     ] Parameters                     params={'observation_shape': [79], 'action_size': 2, 'config': {'type': 'discrete_cql', 'params': {'batch_size': 256, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'compile_graph': False, 'learning_rate': 0.0001, 'optim_factory': {'type': 'adam', 'params': {'clip_grad_norm': None, 'lr_scheduler_factory': {'type': 'none', 'params': {}}, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'n_critics': 1, 'target_update_interval': 8000, 'alpha': 1.0}}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38362e4f5d334d7ba7972cf48150ad28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:45.30 [info     ] loan_cql_run_20251209124518: epoch=1 step=1000 epoch=1 metrics={'time_sample_batch': 0.004452955484390259, 'time_algorithm_update': 0.0071353626251220706, 'loss': 2234.8534659423826, 'td_loss': 2233.859842895508, 'conservative_loss': 0.9936207665205001, 'time_step': 0.011640814304351807} step=1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d530dd852ca4285b306e4fc0e509121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:45.41 [info     ] loan_cql_run_20251209124518: epoch=2 step=2000 epoch=2 metrics={'time_sample_batch': 0.004081830501556396, 'time_algorithm_update': 0.007274195432662964, 'loss': 2227.985461425781, 'td_loss': 2226.917547729492, 'conservative_loss': 1.0679138877987862, 'time_step': 0.011447357416152954} step=2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a494c146a204ce587a9f43e479e7b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:45.53 [info     ] loan_cql_run_20251209124518: epoch=3 step=3000 epoch=3 metrics={'time_sample_batch': 0.0047473304271698, 'time_algorithm_update': 0.006556984901428223, 'loss': 2242.6324913330077, 'td_loss': 2241.5626302490236, 'conservative_loss': 1.0698606004714966, 'time_step': 0.011416449785232544} step=3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac273c92b514c4f9c9dd16203b74b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:46.05 [info     ] loan_cql_run_20251209124518: epoch=4 step=4000 epoch=4 metrics={'time_sample_batch': 0.004105709314346314, 'time_algorithm_update': 0.0071660706996917725, 'loss': 2234.772651489258, 'td_loss': 2233.7125447998046, 'conservative_loss': 1.0601074762940408, 'time_step': 0.011385433197021485} step=4000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91aef33c5fab4e8592b9ce192a6ed7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:46.17 [info     ] loan_cql_run_20251209124518: epoch=5 step=5000 epoch=5 metrics={'time_sample_batch': 0.004933040618896484, 'time_algorithm_update': 0.007271283149719239, 'loss': 2246.772549560547, 'td_loss': 2245.697715698242, 'conservative_loss': 1.0748399902582169, 'time_step': 0.012377523183822632} step=5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b227628bfe264001aefca1821eb78f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:46.29 [info     ] loan_cql_run_20251209124518: epoch=6 step=6000 epoch=6 metrics={'time_sample_batch': 0.004644346714019775, 'time_algorithm_update': 0.00751648998260498, 'loss': 2235.352747558594, 'td_loss': 2234.2765003662107, 'conservative_loss': 1.0762469405531883, 'time_step': 0.012233129262924195} step=6000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf3a1378b61450085fc58639d299038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:46.43 [info     ] loan_cql_run_20251209124518: epoch=7 step=7000 epoch=7 metrics={'time_sample_batch': 0.004695441484451294, 'time_algorithm_update': 0.008285058498382568, 'loss': 2249.717970825195, 'td_loss': 2248.648846435547, 'conservative_loss': 1.0691236688494683, 'time_step': 0.013110782623291016} step=7000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d960839b0c0b4fb7b3d1c5bb3ea2dca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:46.57 [info     ] loan_cql_run_20251209124518: epoch=8 step=8000 epoch=8 metrics={'time_sample_batch': 0.004977902889251709, 'time_algorithm_update': 0.008572404623031616, 'loss': 2240.7507578125, 'td_loss': 2239.6659310302734, 'conservative_loss': 1.084826292693615, 'time_step': 0.01367981219291687} step=8000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a8e02e29dd48c29a1eb12e41e45a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:47.09 [info     ] loan_cql_run_20251209124518: epoch=9 step=9000 epoch=9 metrics={'time_sample_batch': 0.004460729837417602, 'time_algorithm_update': 0.007578107357025146, 'loss': 2252.0469572753905, 'td_loss': 2250.964448364258, 'conservative_loss': 1.0825099548697472, 'time_step': 0.012147035837173461} step=9000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2115938a95c640818415332ad120d110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:47.21 [info     ] loan_cql_run_20251209124518: epoch=10 step=10000 epoch=10 metrics={'time_sample_batch': 0.004477681159973145, 'time_algorithm_update': 0.007120664358139038, 'loss': 2227.849716430664, 'td_loss': 2226.769458984375, 'conservative_loss': 1.080257894217968, 'time_step': 0.011683770418167115} step=10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {'time_sample_batch': 0.004452955484390259,\n",
       "   'time_algorithm_update': 0.0071353626251220706,\n",
       "   'loss': 2234.8534659423826,\n",
       "   'td_loss': 2233.859842895508,\n",
       "   'conservative_loss': 0.9936207665205001,\n",
       "   'time_step': 0.011640814304351807}),\n",
       " (2,\n",
       "  {'time_sample_batch': 0.004081830501556396,\n",
       "   'time_algorithm_update': 0.007274195432662964,\n",
       "   'loss': 2227.985461425781,\n",
       "   'td_loss': 2226.917547729492,\n",
       "   'conservative_loss': 1.0679138877987862,\n",
       "   'time_step': 0.011447357416152954}),\n",
       " (3,\n",
       "  {'time_sample_batch': 0.0047473304271698,\n",
       "   'time_algorithm_update': 0.006556984901428223,\n",
       "   'loss': 2242.6324913330077,\n",
       "   'td_loss': 2241.5626302490236,\n",
       "   'conservative_loss': 1.0698606004714966,\n",
       "   'time_step': 0.011416449785232544}),\n",
       " (4,\n",
       "  {'time_sample_batch': 0.004105709314346314,\n",
       "   'time_algorithm_update': 0.0071660706996917725,\n",
       "   'loss': 2234.772651489258,\n",
       "   'td_loss': 2233.7125447998046,\n",
       "   'conservative_loss': 1.0601074762940408,\n",
       "   'time_step': 0.011385433197021485}),\n",
       " (5,\n",
       "  {'time_sample_batch': 0.004933040618896484,\n",
       "   'time_algorithm_update': 0.007271283149719239,\n",
       "   'loss': 2246.772549560547,\n",
       "   'td_loss': 2245.697715698242,\n",
       "   'conservative_loss': 1.0748399902582169,\n",
       "   'time_step': 0.012377523183822632}),\n",
       " (6,\n",
       "  {'time_sample_batch': 0.004644346714019775,\n",
       "   'time_algorithm_update': 0.00751648998260498,\n",
       "   'loss': 2235.352747558594,\n",
       "   'td_loss': 2234.2765003662107,\n",
       "   'conservative_loss': 1.0762469405531883,\n",
       "   'time_step': 0.012233129262924195}),\n",
       " (7,\n",
       "  {'time_sample_batch': 0.004695441484451294,\n",
       "   'time_algorithm_update': 0.008285058498382568,\n",
       "   'loss': 2249.717970825195,\n",
       "   'td_loss': 2248.648846435547,\n",
       "   'conservative_loss': 1.0691236688494683,\n",
       "   'time_step': 0.013110782623291016}),\n",
       " (8,\n",
       "  {'time_sample_batch': 0.004977902889251709,\n",
       "   'time_algorithm_update': 0.008572404623031616,\n",
       "   'loss': 2240.7507578125,\n",
       "   'td_loss': 2239.6659310302734,\n",
       "   'conservative_loss': 1.084826292693615,\n",
       "   'time_step': 0.01367981219291687}),\n",
       " (9,\n",
       "  {'time_sample_batch': 0.004460729837417602,\n",
       "   'time_algorithm_update': 0.007578107357025146,\n",
       "   'loss': 2252.0469572753905,\n",
       "   'td_loss': 2250.964448364258,\n",
       "   'conservative_loss': 1.0825099548697472,\n",
       "   'time_step': 0.012147035837173461}),\n",
       " (10,\n",
       "  {'time_sample_batch': 0.004477681159973145,\n",
       "   'time_algorithm_update': 0.007120664358139038,\n",
       "   'loss': 2227.849716430664,\n",
       "   'td_loss': 2226.769458984375,\n",
       "   'conservative_loss': 1.080257894217968,\n",
       "   'time_step': 0.011683770418167115})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cql.fit(\n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    save_interval=1000,\n",
    "    #eval_episodes=None, # don't have a gym environment to evaluate live\n",
    "    experiment_name=\"loan_cql_run\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "812f5b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to cql_loan_agent.pt\n"
     ]
    }
   ],
   "source": [
    "cql.save_model(\"cql_loan_agent.pt\")\n",
    "print(\"Model saved to cql_loan_agent.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca5dce",
   "metadata": {},
   "source": [
    "# Evaluating the RL model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c2aa5",
   "metadata": {},
   "source": [
    "<p>1. Select ONLY the Real Historical Data (Batch A) <br>\n",
    "In Step 2, we concatenated [Real, Synthetic]. The first half of the dataset is the Real data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f236d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_real = len(df)\n",
    "real_observations = observations[:n_real]\n",
    "real_rewards = rewards[:n_real]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af2281",
   "metadata": {},
   "source": [
    "<p>2. Ask the RL Agent to make decisions</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46e483e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict() returns the action (0 or 1) for each state\n",
    "rl_actions = cql.predict(real_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0e6cb",
   "metadata": {},
   "source": [
    "<p>3. Calculate the RL values</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80b2427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_portfolio_rewards = []\n",
    "rl_approve_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76ca0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_real):\n",
    "    action = rl_actions[i]\n",
    "    \n",
    "    if action == 0: # Agent Denies\n",
    "        rl_portfolio_rewards.append(0) # Safe, no gain/loss\n",
    "    else: # Agent Approves\n",
    "        # We get the ACTUAL historical result\n",
    "        rl_portfolio_rewards.append(real_rewards[i])\n",
    "        rl_approve_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b2012",
   "metadata": {},
   "source": [
    "## Compare Human vs RL-based policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "161de988",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_avg_reward = np.mean(real_rewards) # actual\n",
    "rl_avg_reward = np.mean(rl_portfolio_rewards) # What RL would have done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "308ecd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Banker Avg Reward:   $-1805.50\n",
      "RL Agent Avg Reward:       $-1325.56\n",
      "----------------------------------------\n",
      "Human Approval Rate:       100.0% (By definition of dataset's subset)\n",
      "RL Agent Approval Rate:    95.2%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Human Banker Avg Reward:   ${human_avg_reward:.2f}\")\n",
    "print(f\"RL Agent Avg Reward:       ${rl_avg_reward:.2f}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Human Approval Rate:       100.0% (By definition of dataset's subset)\")\n",
    "print(f\"RL Agent Approval Rate:    {rl_approve_count / n_real * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5c0ae",
   "metadata": {},
   "source": [
    "## Total Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d741e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Portfolio Improvement: $84,510,026.31\n"
     ]
    }
   ],
   "source": [
    "total_gain = (rl_avg_reward - human_avg_reward) * n_real\n",
    "print(f\"Total Portfolio Improvement: ${total_gain:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc16304",
   "metadata": {},
   "source": [
    "# Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c773e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Comparison Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Human Policy</th>\n",
       "      <th>RL Agent Policy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Average Reward per Loan</td>\n",
       "      <td>-1.805502e+03</td>\n",
       "      <td>-1.325558e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total Portfolio Value</td>\n",
       "      <td>-3.179182e+08</td>\n",
       "      <td>-2.334082e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Metric  Human Policy  RL Agent Policy\n",
       "0  Average Reward per Loan -1.805502e+03    -1.325558e+03\n",
       "1    Total Portfolio Value -3.179182e+08    -2.334082e+08"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Average Reward per Loan', 'Total Portfolio Value'],\n",
    "    'Human Policy': [human_avg_reward, np.sum(real_rewards)],\n",
    "    'RL Agent Policy': [rl_avg_reward, np.sum(rl_portfolio_rewards)]\n",
    "})\n",
    "print(\"\\nFinal Comparison Table:\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf8081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

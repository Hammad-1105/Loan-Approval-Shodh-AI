# Shodh-AI: Loan Approval Policy Optimization  
**Deep Learning vs Offline Reinforcement Learning for Credit Risk**

Author: **Hammad Shaikh**  
Assessment: **Shodh AI â€“ Machine Learning Internship Task**  
Report: `Hammad_Shaikh_Report_Shodh_AI.pdf`

---

## ðŸ“Œ Executive Summary

This project explores the transition from **predicting credit risk** to **optimizing financial returns**.  
Using the LendingClub dataset (2007â€“2018), I designed and compared two AI systems:

1. **Supervised Deep Learning (DL):** Predicts probability of default.  
2. **Offline Reinforcement Learning (RL):** A Conservative Q-Learning (CQL) agent that optimizes portfolio value.

### **Core Finding**
While both models beat the human baseline, the **Optimized DL Model** was superior:

- DL Policy reduced losses by **85%**
- RL Policy reduced losses by **26%**

This shows that **a well-tuned classifier can outperform offline RL when the â€œsafeâ€ action (Deny) is well-defined.**

---

## ðŸ“‚ Repository Structure

```bash
models/
 â””â”€â”€ cql_loan_agent.pt                # Trained RL agent weights

notebooks/
 â”œâ”€â”€ 01_data_preprocessing.ipynb      # Cleaning, leakage removal, scaling
 â”œâ”€â”€ 02_deep_learning.ipynb           # MLP training + threshold optimization
 â”œâ”€â”€ 03_offline_rl.ipynb              # Reward engineering + CQL agent
 â””â”€â”€ 04_analysis_and_comparison.ipynb # EPV analysis + policy divergence

README.md
requirements.txt
Hammad_Shaikh_Report_Shodh_AI.pdf     # Final 3-page report

âš™ï¸ 1. Environment Setup
1.1 Clone the repository
git clone https://github.com/Hammad-1105/Loan-Approval-Shodh-AI.git
cd Loan-Approval-Shodh-AI

1.2 Create a virtual environment
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows

1.3 Install dependencies
pip install -r requirements.txt

ðŸ“Š 2. Dataset Requirement

Download the LendingClub dataset:

accepted_2007_to_2018.csv


Place it inside:

data/accepted_2007_to_2018.csv


The notebooks automatically load it from this path.

â–¶ï¸ 3. How to Reproduce All Results

Run the notebooks in order:

STEP 1 â€” Data Preprocessing

ðŸ“Œ Notebook: notebooks/01_data_preprocessing.ipynb

This notebook:

Removes 58 columns with >30% missing data

Drops leakage features

Performs one-hot encoding

Applies StandardScaler

Removes highly correlated features

Produces a final cleaned dataset (61 features)

Output: cleaned_data.csv

STEP 2 â€” Deep Learning Classifier

ðŸ“Œ Notebook: notebooks/02_deep_learning.ipynb

This notebook:

Trains the MLP (256 â†’ 128 â†’ 64 + dropout)

Handles class imbalance

Performs threshold tuning

Reproduced Metrics:

Metric	Value
AUC	0.741
F1-score	0.456
Approval Rate	65.6%
EPV	â€“$273.19
STEP 3 â€” Offline RL (CQL Agent)

ðŸ“Œ Notebook: notebooks/03_offline_rl.ipynb

This notebook:

Creates counterfactual Deny actions (reward = 0)

Defines reward:

Approve + Paid â†’ +interest

Approve + Default â†’ â€“loan amount

Trains Conservative Q-Learning

Evaluates EPV

Reproduced Metrics:

Metric	Value
Approval Rate	95.2%
EPV	â€“$1,339.02
STEP 4 â€” Final Analysis & Comparison

ðŸ“Œ Notebook: notebooks/04_analysis_and_comparison.ipynb

This notebook:

Computes EPV for human, DL model, RL agent

Evaluates strict vs lenient policies

Analyzes divergence cases

Recreates Applicant #45 example (DL denies, RL approves â†’ default)

Final Results Table (Task 4.1)
Policy	EPV per loan	% Improvement vs Human
Human Baseline	â€“$1,805.50	â€“
RL Agent (CQL)	â€“$1,339.02	+26%
Deep Learning (MLP)	â€“$273.19	+85%
ðŸ“Œ 4. Reproducibility Notes

All results in the final report are generated by these notebooks.

Running the four notebooks sequentially will reproduce:

AUC, F1

RL EPV

Divergent case examples

Final improvements table